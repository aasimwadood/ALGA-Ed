{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d4adfa",
   "metadata": {},
   "source": [
    "# ALGA-Ed Notebook\n",
    "Includes dataset loading, preprocessing, synthetic data generation, RL training, DALL·E image generation, Whisper transcription, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43226e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai transformers torch pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b806a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import Image, display\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = 'YOUR_OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a9467",
   "metadata": {},
   "source": [
    "## Load and Preprocess EdNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_ednet(file_path='ednet_data/KT1/problem_log.csv'):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"EdNet file not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset=['elapsed_time', 'correct'])\n",
    "    df['elapsed_time'] = df['elapsed_time'].clip(0, 60000)\n",
    "    scaler = MinMaxScaler()\n",
    "    df[['elapsed_time']] = scaler.fit_transform(df[['elapsed_time']])\n",
    "    user_stats = df.groupby('user_id')['correct'].mean().reset_index()\n",
    "    user_stats.columns = ['user_id', 'accuracy']\n",
    "    df = df.merge(user_stats, on='user_id')\n",
    "    df['error_rate'] = 1 - df['accuracy']\n",
    "    return df[['elapsed_time', 'accuracy', 'error_rate', 'correct']]\n",
    "\n",
    "# ednet_data = preprocess_ednet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1b450",
   "metadata": {},
   "source": [
    "##  Load and Preprocess ASSISTments Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46012f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_assistments(file_path='assistments_2009.csv'):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"ASSISTments file not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset=['hint_count', 'correct'])\n",
    "    df['hint_count'] = df['hint_count'].clip(0, 10)\n",
    "    df['time'] = df['time'].fillna(100).clip(0, 10000)\n",
    "    scaler = MinMaxScaler()\n",
    "    df[['hint_count', 'time']] = scaler.fit_transform(df[['hint_count', 'time']])\n",
    "    student_stats = df.groupby('student_id')['correct'].mean().reset_index()\n",
    "    student_stats.columns = ['student_id', 'accuracy']\n",
    "    df = df.merge(student_stats, on='student_id')\n",
    "    df['error_rate'] = 1 - df['accuracy']\n",
    "    return df[['hint_count', 'time', 'accuracy', 'error_rate', 'correct']]\n",
    "\n",
    "# assist_data = preprocess_assistments()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a720b04",
   "metadata": {},
   "source": [
    "## Synthetic Dataset for Disabled Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_synthetic_disabled_data(n_samples=1000):\n",
    "    np.random.seed(42)\n",
    "    profiles = []\n",
    "    for _ in range(n_samples):\n",
    "        disability = np.random.choice(['ADHD', 'dyslexia', 'visual_impairment'], p=[0.4, 0.4, 0.2])\n",
    "        if disability == 'ADHD':\n",
    "            profile = {'elapsed_time': np.random.uniform(0.6, 1.0),\n",
    "                       'error_rate': np.random.uniform(0.3, 0.6),\n",
    "                       'hint_usage': np.random.uniform(0.2, 0.5),\n",
    "                       'disability': 'ADHD'}\n",
    "        elif disability == 'dyslexia':\n",
    "            profile = {'elapsed_time': np.random.uniform(0.7, 1.0),\n",
    "                       'error_rate': np.random.uniform(0.4, 0.7),\n",
    "                       'hint_usage': np.random.uniform(0.4, 0.7),\n",
    "                       'disability': 'dyslexia'}\n",
    "        else:\n",
    "            profile = {'elapsed_time': np.random.uniform(0.8, 1.0),\n",
    "                       'error_rate': np.random.uniform(0.2, 0.4),\n",
    "                       'hint_usage': np.random.uniform(0.6, 0.9),\n",
    "                       'disability': 'visual_impairment'}\n",
    "        profiles.append(profile)\n",
    "    return pd.DataFrame(profiles)\n",
    "\n",
    "synthetic_df = generate_synthetic_disabled_data()\n",
    "synthetic_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e3259",
   "metadata": {},
   "source": [
    "## Visualizations of Engagement by Disability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2586a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='disability', y='elapsed_time', data=synthetic_df, palette='Set2')\n",
    "plt.title('Elapsed Time by Disability')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(x='disability', y='error_rate', data=synthetic_df, palette='Set3')\n",
    "plt.title('Error Rate by Disability')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(x='disability', y='hint_usage', data=synthetic_df, palette='Set1')\n",
    "plt.title('Hint Usage by Disability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Engagement Profiles by Disability', fontsize=16, y=1.03)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88809875",
   "metadata": {},
   "source": [
    "## GPT2 Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(inputs['input_ids'], max_length=100)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "tg = TextGenerator()\n",
    "print(tg.generate(\"Explain gravity to a 10-year-old\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631fb05",
   "metadata": {},
   "source": [
    "## PyTorch RL Agent and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be821c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedbackAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(FeedbackAgent, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "agent = FeedbackAgent(3, 2)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "data = [\n",
    "    ([0.6, 0.2, 0.8], [0.7, 0.3]),\n",
    "    ([0.9, 0.1, 0.9], [0.95, 0.05]),\n",
    "    ([0.4, 0.3, 0.5], [0.5, 0.5]),\n",
    "    ([0.2, 0.5, 0.3], [0.3, 0.7]),\n",
    "]\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for state_vals, target_vals in data:\n",
    "        state = torch.tensor(state_vals, dtype=torch.float32)\n",
    "        target = torch.tensor(target_vals, dtype=torch.float32)\n",
    "        output = agent(state)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Test output:\", agent(torch.tensor([0.8, 0.2, 0.9])).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701cd18",
   "metadata": {},
   "source": [
    "## DALL·E Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = openai.Image.create(\n",
    "  prompt=\"high contrast diagram of the solar system for visually impaired students\",\n",
    "  n=1,\n",
    "  size=\"512x512\"\n",
    ")\n",
    "image_url = response['data'][0]['url']\n",
    "display(Image(url=image_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e85fc",
   "metadata": {},
   "source": [
    "## Whisper Audio Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa02a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fname in uploaded.keys():\n",
    "    with open(fname, \"rb\") as audio_file:\n",
    "        transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "        print(f\"Transcript for {fname}:\", transcript[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440150b7",
   "metadata": {},
   "source": [
    "## Test Cases for ALGA-Ed System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3acb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dummy functions (to be replaced by actual implementation functions)\n",
    "def preprocess_data(df): return df.fillna(0)\n",
    "def normalize_features(df): return (df - df.min()) / (df.max() - df.min())\n",
    "def simulate_ednet_data(n): return pd.DataFrame({'response_time': np.random.rand(n), 'correct': np.random.randint(0, 2, n), 'attempts': np.random.randint(1, 4, n)})\n",
    "def extract_features(df): return df[['response_time', 'correct', 'attempts']]\n",
    "def generate_synthetic_data(num_samples): return pd.DataFrame({'disability_type': np.random.choice(['ADHD', 'dyslexia'], num_samples)})\n",
    "def compute_reward(old, new): return max(0, new - old)\n",
    "def rl_policy(state): return int(np.argmax(state))\n",
    "def update_profile(profile, feedback): profile.update(feedback); return profile\n",
    "def simplify_text(text): return \"Simple: \" + text.split('.')[0]\n",
    "def generate_educational_visual(prompt): return \"Generated_Image_URL\"\n",
    "\n",
    "# Test cases\n",
    "def test_missing_values_handling():\n",
    "    df = pd.DataFrame({\"score\": [85, np.nan, 70]})\n",
    "    processed = preprocess_data(df)\n",
    "    assert not processed.isnull().values.any()\n",
    "\n",
    "def test_normalization_range():\n",
    "    df = pd.DataFrame({\"engagement_time\": [10, 20, 30]})\n",
    "    norm_df = normalize_features(df)\n",
    "    assert norm_df[\"engagement_time\"].between(0, 1).all()\n",
    "\n",
    "def test_feature_shape():\n",
    "    raw_data = simulate_ednet_data(10)\n",
    "    features = extract_features(raw_data)\n",
    "    assert features.shape[0] == 10\n",
    "\n",
    "def test_feature_columns_exist():\n",
    "    features = extract_features(simulate_ednet_data(5))\n",
    "    expected = {\"response_time\", \"correct\", \"attempts\"}\n",
    "    assert expected.issubset(set(features.columns))\n",
    "\n",
    "def test_synthetic_behavior_distribution():\n",
    "    data = generate_synthetic_data(num_samples=100)\n",
    "    assert data['disability_type'].nunique() > 1\n",
    "\n",
    "def test_synthetic_data_types():\n",
    "    data = generate_synthetic_data(20)\n",
    "    assert data.select_dtypes(include=['number']).shape[1] >= 0\n",
    "\n",
    "def test_reward_positive_for_improvement():\n",
    "    assert compute_reward(50, 70) > 0\n",
    "\n",
    "def test_rl_policy_returns_action():\n",
    "    assert isinstance(rl_policy(np.array([0.2, 0.5, 0.7])), int)\n",
    "\n",
    "def test_feedback_updates_profile():\n",
    "    profile = {\"level\": \"intermediate\"}\n",
    "    updated = update_profile(profile, {\"mistakes\": 3})\n",
    "    assert \"mistakes\" in updated\n",
    "\n",
    "def test_text_simplification():\n",
    "    assert len(simplify_text(\"Photosynthesis is a biochemical process.\").split()) < 10\n",
    "\n",
    "def test_image_generation():\n",
    "    assert generate_educational_visual(\"basic algebra\") is not None\n",
    "\n",
    "def test_pipeline_flow():\n",
    "    raw = simulate_ednet_data(20)\n",
    "    pre = preprocess_data(raw)\n",
    "    feat = extract_features(pre)\n",
    "    result = {\"accuracy\": feat['correct'].mean()}\n",
    "    assert result[\"accuracy\"] >= 0\n",
    "\n",
    "print(\"All test cases defined. Run using pytest or call individually.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
